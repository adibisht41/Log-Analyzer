# Log-Analyzer

Introduction:
Log analyzer is a tool to analyse and generate information from log files, through the
technique called log analysis.
Production operations generate all styles of logs: server logs, OS logs, application logs, etc.
We require this stuff, gather them up, treat them as knowledge, and use them somehow. It
is widely used because many a times log file have data in unstructured manner.
So log analysis takes you from "unstructured" to "ready to form smart choices."


Assumptions:
1) Due to memory error on using Windows command prompt, preferably use
Anaconda prompt or IPython console in Spyder.
2) The log data provided to us is in the form of text file (.txt)
3) The data is having a specific format, as provided in the sample given.
4) Each log entry is line seperated and each log entry contains a delimiter ‘#,!’ in
between each key value.
5) The output data is in JSON and Database format to be used by the user.
2


Approach Taken:

1) create_big_file.py
The approach takes negligible time, when we execute our program, on file of small size. But
in real world, the analysis needs to be done on file data of about 1-2 GB. create_big_file.py
is the python script to create testing data file of size of 1 GB or more, according to the
parameters passed.

2) log_analyer_JSON.py
1) Import required python packages like time, json, re
2) Start a timer and store the starting time in StartTime variable
3) Create a list “Keys” having the values of different keys in the log file. In the given case
we have 13 different key values.
3
'IP Address',
'User Agent',
'X Request From',
'Request Type',
'API',
'User Login',
'User Name',
'EnterpriseId',
'EnterpriseName',
'Auth Status',
'Status Code',
'Response Time',
'Request Body'
4) We start reading the log file and store the value of each line in “Text”.
5) Compiling the regular expression into object. Searching the key values in the given
text using regular expression.
6) Start an iterator and store the value of keys in the list “Values”. Value of particular key
can be obtained using given formulae:
Starting index of current key + Length of current key : starting index of next key
7) Now store the value corresponding to each key in a dictionary.
8) We finally convert the dictionary data to json output file (.json).
3) log_analzyer_db.py
1) Import required python packages like time, json, re
2) Start a timer and store the starting time in StartTime variable
3) Create a list “Keys” having the values of different keys in the log file. In the given case
we have 13 different key values.
'IP Address',
'User Agent',
'X Request From',
'Request Type',
4
'API',
'User Login',
'User Name',
'EnterpriseId',
'EnterpriseName',
'Auth Status',
'Status Code',
'Response Time',
'Request Body'
4) We start reading the log file and store the value of each line in “Text”.
5) Compiling the regular expression into object. Searching the key values in the given
text using regular expression.
6) Start an iterator and store the value of keys in the list “Values”. Value of particular key
can be obtained using given formulae:
Starting index of current key + Length of current key : starting index of next key
7) Now store the value corresponding to each key in a dictionary.
8) Using sqlite database, we will create a table having column names as the items in the
“Keys” list.
9) Finally inserting data from dictionary into table logs and thus generating a .db file.
Output Data
The output data should be useful for the programmer. So the data is generated in the form
of 2 files:

1) JSON formatted file (.json)
JavaScript Object Notation is a light-weight format for data interchange. Easy for
humans to write and read and easy for machines to generate and parse the data in
JSON format. It is text format that is completely language independent.
This formatted data can be used by programmers for javascript based applications, that
can be used for web browser extensions and websites, to provide public data using web
5
servers and APIs. It is basically used to transmit data between web servers and
applications.
Output of log_analyzer_json.py on an online json viewer
2) Database format (.db)
The data is stored in row column format, organized into tables and fields. Each value that is
stored in the table is called record. The .db file generated by log analyzer will contain a
table, which will have the key values like IP-Address as the column name and the record will
be the corresponding value of IP-address like 147.49.141.133. It is generally used for
storing data, referenced by dynamic websites.
6
NOTE: Size of output file is half of the size of the input file
Output for log_analyzer_db.py


Improvements:
1) The speed of processing can be improved by multiprocessing.
2) A UI/UX can be created, thus developing an application, where input will be a text
file having log data and output will be .json or .db files, to be viewed on the
application.
3) Different graphs and charts can also be built and analysed to give insight of the log
data.
How to run the code
1) Run create_big_file.py. It will create a 1 GB log data file.
2) Run log_analyzer_json.py. It will generate a json file.
3) Similarly run log_analyzer_db.py. It will create a .db file, which contains the records
in the form sql table, which can be viewed on any sqlite viewer softwares, example
“DB Browser for SQLite.”
7


Results:
1) Time taken for processing log_analyzer_db.py on
1 GB file = 80 seconds (1.3 minutes) approx.
2 GB file= 320 seconds ( 5.3 minutes) approx.
Size of output file is half of the size of the input file.
2) Time taken for processing log_analyzer_json.py on
1 GB file = 120 - 180 seconds (2 - 3 minutes) approx
